{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVbpZc0a53/Z27bixq151c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Drive Hesabına Erişim"],"metadata":{"id":"xfvDlU8cRJbb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRNdhHL7Qj7o","executionInfo":{"status":"ok","timestamp":1680636568908,"user_tz":-180,"elapsed":31184,"user":{"displayName":"nisa esma","userId":"00452795290100062143"}},"outputId":"89c0f301-8934-4304-e987-46de01f833d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","except:\n","    print(\"You are not working in Colab at the moment :(\")"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/TDDİY'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRfNVNT3Q-TP","executionInfo":{"status":"ok","timestamp":1680636568909,"user_tz":-180,"elapsed":22,"user":{"displayName":"nisa esma","userId":"00452795290100062143"}},"outputId":"bac9fd98-dc54-43cf-f2d0-8f818b134c36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/18TnDAyZYuScf8Jlx-5YfmnreW4TxwRwf/TDDİY\n"]}]},{"cell_type":"markdown","source":["## Kütüphanelerin Yüklenmesi"],"metadata":{"id":"sCpZWZEzRE4Q"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd \n","import nltk\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","\n","import os\n","import re"],"metadata":{"id":"f97YkQblQ--w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Verinin yüklenmesi"],"metadata":{"id":"_AnF5IWySL7m"}},{"cell_type":"code","source":["test_set = pd.read_csv('test_set.csv', sep=\"|\")"],"metadata":{"id":"vdGwFl_GRYKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_set.head()"],"metadata":{"id":"ItrdMsOd5Ws2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Veri Önişleme"],"metadata":{"id":"PVhz-XKJApLd"}},{"cell_type":"code","source":["!pip install jpype1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2h3pIRXZy08G","executionInfo":{"status":"ok","timestamp":1679600316449,"user_tz":-180,"elapsed":2574,"user":{"displayName":"Eğitim Birads","userId":"16983725387962807471"}},"outputId":"45438e2a-1e9f-421b-94b2-b6a3a6cb752d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: jpype1 in /usr/local/lib/python3.9/dist-packages (1.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from jpype1) (23.0)\n"]}]},{"cell_type":"code","source":["import string\n","import gensim.parsing.preprocessing as gsp\n","import re\n","from typing import List\n","from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n","ZEMBEREK_PATH = '/content/drive/MyDrive/TDDİY/zemberek-full.jar'"],"metadata":{"id":"uCzBFOT9A1cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# noktalama işaretlerinin kaldırılması\n","def remove_punctuation(data):\n","    data_no_punctuation = [ch for ch in str(data) if ch not in string.punctuation]\n","    data_no_punctuation = \"\".join(data_no_punctuation)\n","\n","    return data_no_punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","\n","# durak kelimelerin kaldırılması\n","def remove_stopwords(text): \n","    stop_words = ['çok', 'at', 'in', 'im', 'acaba','acep','allah','adamakıllı','adeta','ait','altmýþ','altmış','altý'\n","                  , 'altı','ama','amma','anca','ancak','arada','artýk','aslında','aynen','ayrıca','az','açıkça','açıkçası',\n","                  'bana','bari','bazen','bazý','bazı','başkası','baţka','belki','ben','benden','beni','benim',\n","                  'beri','beriki','beþ','beş','beţ','bilcümle','bile','bin','binaen','binaenaleyh','bir','biraz',\n","                  'birazdan','birbiri','birden','birdenbire','biri','birice','birileri','birisi','birkaç',\n","                  'birkaçı','birkez','birlikte','birçok','birçoğu','birþey','birþeyi','birşey','birşeyi','birţey',\n","                  'bitevi','biteviye','bittabi','biz','bizatihi','bizce','bizcileyin','bizden','bize','bizi','bizim',\n","                  'bizimki','bizzat','boşuna','bu','buna','bunda','bundan','bunlar','bunları','bunların','bunu','bunun',\n","                  'buracıkta','burada','buradan','burası','böyle','böylece','böylecene','böylelikle','böylemesine',\n","                  'böylesine','büsbütün','bütün','cuk','cümlesi','da','daha','dahi','dahil','dahilen','daima','dair',\n","                  'dayanarak','de','defa','dek','demin','demincek','deminden','denli','derakap','derhal','derken',\n","                  'deđil','değil','değin','diye','diđer','diğer','diğeri','doksan','dokuz','dolayı','dolayısıyla',\n","                  'doğru','dört','edecek','eden','ederek','edilecek','ediliyor','edilmesi','ediyor','elbet',\n","                  'elbette','elli','emme','en','enikonu','epey','epeyce','epeyi','esasen','esnasında','etmesi',\n","                  'etraflı','etraflıca','etti','ettiği','ettiğini','evleviyetle','evvel','evvela','evvelce',\n","                  'evvelden','evvelemirde','evveli','eđer','eğer','fakat','filanca','gah','gayet','gayetle','gayri',\n","                  'gayrı','gelgelelim','gene','gerek','gerçi','geçende','geçenlerde','gibi','gibilerden','gibisinden',\n","                  'gine','göre','gırla','hakeza','halbuki','halen','halihazırda','haliyle','handiyse','hangi','hangisi',\n","                  'hani','hariç','hasebiyle','hasılı','hatta','hele','hem','henüz','hep','hepsi','her',\n","                  'herhangi','herkes','herkesin','hiç','hiçbir','hiçbiri','hoş','hulasaten','iken','iki',\n","                  'ila','ile','ilen','ilgili','ilk','illa','illaki','imdi','indinde','inen','insermi','ise',\n","                  'ister','itibaren','itibariyle','itibarıyla','iyi','iyice','iyicene','için','iş','işte',\n","                  'iţte','kadar','kaffesi','kah','kala','kanýmca','karşın','katrilyon','kaynak','kaçı','kelli',\n","                  'kendi','kendilerine','kendini','kendisi','kendisine','kendisini','kere','kez','keza','kezalik',\n","                  'keşke','keţke','ki','kim','kimden','kime','kimi','kimisi','kimse','kimsecik','kimsecikler',\n","                  'külliyen','kýrk','kýsaca','kırk','kısaca','lakin','leh','lütfen','maada','madem','mademki',\n","                  'mamafih','mebni','međer','meğer','meğerki','meğerse','milyar','milyon','mu','mü','mý','mı',\n","                  'nasýl','nasıl','nasılsa','nazaran','naşi','ne','neden','nedeniyle','nedenle','nedense',\n","                  'nerde','nerden','nerdeyse','nere','nerede','nereden','neredeyse','neresi','nereye',\n","                  'netekim','neye','neyi','neyse','nice','nihayet','nihayetinde','nitekim','niye','niçin',\n","                  'o','olan','olarak','oldu','olduklarını','oldukça','olduğu','olduğunu','olmadı',\n","                  'olmadığı','olmak','olması','olmayan','olmaz','olsa','olsun','olup','olur','olursa','oluyor',\n","                  'on','ona','onca','onculayın','onda','ondan','onlar','onlardan','onlari','onlarýn','onları',\n","                  'onların','onu','onun','oracık','oracıkta','orada','oradan','oranca','oranla','oraya','otuz',\n","                  'oysa','oysaki','pek','pekala','peki','pekçe','peyderpey','rağmen','sadece','sahi','sahiden',\n","                  'sana','sanki','sekiz','seksen','sen','senden','seni','senin','siz','sizden','sizi','sizin',\n","                  'sonra','sonradan','sonraları','sonunda','tabii','tam','tamam','tamamen','tamamıyla','tarafından',\n","                  'tek','trilyon','tüm','var','vardı','vasıtasıyla','ve','velev','velhasıl','velhasılıkelam','veya',\n","                  'veyahut','ya','yahut','yakinen','yakında','yakından','yakınlarda','yalnız','yalnızca','yani',\n","                  'yapacak','yapmak','yaptı','yaptıkları','yaptığı','yaptığını','yapılan','yapılması','yapıyor',\n","                  'yedi','yeniden','yenilerde','yerine','yetmiþ','yetmiş','yetmiţ','yine','yirmi','yok','yoksa',\n","                  'yoluyla','yüz','yüzünden','zarfında','zaten','zati','zira','çabuk','çabukça','çeşitli',\n","                  'çok','çokları','çoklarınca','çokluk','çoklukla','çokça','çoğu','çoğun','çoğunca','çoğunlukla',\n","                  'çünkü','öbür','öbürkü','öbürü','önce','önceden','önceleri','öncelikle','öteki','ötekisi','öyle',\n","                  'öylece','öylelikle','öylemesine','öz','üzere','üç','þey','þeyden','þeyi','þeyler','þu','þuna',\n","                  'þunda','þundan','þunu','şayet','şey','şeyden','şeyi','şeyler','şu','şuna','şuncacık','şunda',\n","                  'şundan','şunlar','şunları','şunu','şunun','şura','şuracık','şuracıkta','şurası','şöyle',\n","                  'ţayet','ţimdi','ţu','ţöyle', 'hala', 'yer', 'güzel', 'büyük']\n","    stop_words = ['a','acaba','altı','altmış','ama','ancak','arada','artık','asla','aslında','aslında','ayrıca',\n","                  'az','bana','bazen','bazı','bazıları','belki','ben','benden','beni','benim','beri','beş',\n","                  'bile','bilhassa','bin','bir','biraz','birçoğu','birçok','biri','birisi','birkaç','birşey',\n","                  'biz','bizden','bize','bizi','bizim','böyle','böylece','bu','buna','bunda','bundan','bunlar',\n","                  'bunları','bunların','bunu','bunun','burada','bütün','çoğu','çoğunu','çok','çünkü','da',\n","                  'daha','dahi','dan','de','defa','değil','diğer','diğeri','diğerleri','diye','doksan','dokuz',\n","                  'dolayı','dolayısıyla','dört','e','edecek','eden','ederek','edilecek','ediliyor','edilmesi',\n","                  'ediyor','eğer','elbette','elli','en','etmesi','etti','ettiği','ettiğini','fakat','falan',\n","                  'filan','gene','gereği','gerek','gibi','göre','hala','halde','halen','hangi','hangisi',\n","                  'hani','hatta','hem','henüz','hep','hepsi','her','herhangi','herkes','herkese','herkesi',\n","                  'herkesin','hiç','hiçbir','hiçbiri','i','ı','için','içinde','iki','ile','ilgili','ise',\n","                  'işte','itibaren','itibariyle','kaç','kadar','karşın','kendi','kendilerine','kendine',\n","                  'kendini','kendisi','kendisine','kendisini','kez','ki','kim','kime','kimi','kimin',\n","                  'kimisi','kimse','kırk','madem','mi','mı','milyar','milyon','mu','mü','nasıl','ne',\n","                  'neden','nedenle','nerde','nerede','nereye','neyse','niçin','nin','nın','niye','nun',\n","                  'nün','o','öbür','olan','olarak','oldu','olduğu','olduğunu','olduklarını','olmadı',\n","                  'olmadığı','olmak','olması','olmayan','olmaz','olsa','olsun','olup','olur','olur','olursa',\n","                  'oluyor','on','ön','ona','önce','ondan','onlar','onlara','onlardan','onları','onların',\n","                  'onu','onun','orada','öte','ötürü','otuz','öyle','oysa','pek','rağmen','sana','sanki',\n","                  'sanki','şayet','şekilde','sekiz','seksen','sen','senden','seni','senin','şey','şeyden',\n","                  'şeye','şeyi','şeyler','şimdi','siz','siz','sizden','sizden','size','sizi','sizi',\n","                  'sizin','sizin','sonra','şöyle','şu','şuna','şunları','şunu','ta','tabii','tam',\n","                  'tamam','tamamen','tarafından','trilyon','tüm','tümü','u','ü','üç','un','ün','üzere',\n","                  'var','vardı','ve','veya','ya','yani','yapacak','yapılan','yapılması','yapıyor','yapmak',\n","                  'yaptı','yaptığı','yaptığını','yaptıkları','ye','yedi','yerine','yetmiş','yi','yı','yine',\n","                  'yirmi','yoksa','yu','yüz','zaten','zira','zxtest']\n","    \n","    stop_words = ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'bir', 'birkaç', 'birşey', 'biz',\n","                  'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'den', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem',\n","                  'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü',\n","                  'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey',\n","                  'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani', 'dan']\n","    word_tokens = nltk.word_tokenize(text) \n","    filtered_text = [word for word in word_tokens if word not in stop_words] \n","    return ' '.join(filtered_text)\n","\n","# Bazı Türkçe olmayan harflerin türkçe karşılıklarına çevrilmesi\n","def correct_old_characters(self):\n","    self = re.sub(r\"Â\", \"A\", self)\n","    self = re.sub(r\"Î\", \"I\", self)\n","    self = re.sub(r\"î\", \"ı\", self)\n","    self = re.sub(r\"â\", \"a\", self)\n","    self = re.sub(r\"û\", \"u\", self)\n","    self = re.sub(r\"Û\", \"U\", self)\n","    return self\n","\n","# İki karakterden küçük kelimelerin kaldırılması\n","def remove_two_ch_words(data):\n","  text = ' '.join([w for w in data.split() if len(w)>2])\n","  return text\n","\n","# Kelimelerin küçük harfe çevrilmesi\n","def lower(self):\n","    self = re.sub(r\"İ\", \"i\", self)\n","    self = re.sub(r\"I\", \"ı\", self)\n","    self = re.sub(r\"Ç\", \"ç\", self)\n","    self = re.sub(r\"Ş\", \"ş\", self)\n","    self = re.sub(r\"Ü\", \"ü\", self)\n","    self = re.sub(r\"Ğ\", \"ğ\", self)\n","    self = self.lower() # for the rest use default lower\n","    return self\n","\n","# Kelimelerin lemmalarının bulunması\n","startJVM(jvmpath=getDefaultJVMPath(), classpath= ZEMBEREK_PATH, convertStrings=False, interrupt=True)\n","def tokenizasyon(text):\n","    return word_tokenize(text)\n","    \n","def lemmatizer(text):\n","    lemma_words = []\n","    global counter\n","    TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n","    morphology = TurkishMorphology.createWithDefaults()\n","    for text in tokenizasyon(text):\n","      lemma_word = str(morphology.analyzeAndDisambiguate(str(text)).bestAnalysis()[0].getLemmas()[0])\n","      lemma_words.append(lemma_word)\n","    \n","    text = ' '.join(lemma_words)\n","\n","    return text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsXB3cyUA3JZ","executionInfo":{"status":"ok","timestamp":1679600316895,"user_tz":-180,"elapsed":451,"user":{"displayName":"Eğitim Birads","userId":"16983725387962807471"}},"outputId":"2189ff1f-208a-4b22-e9b7-5375a805e298"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["test_set['remove_punctuation'] = test_set['text'].apply(remove_punctuation)\n","test_set['remove_stopwords'] = test_set['remove_punctuation'].apply(remove_stopwords)\n","regex_for_numbers = r'([0-9]+)'\n","test_set['remove_numbers'] = test_set['remove_stopwords'].str.replace(regex_for_numbers, ' ', case=False, regex=True)\n","test_set['correct_chars'] = test_set['remove_numbers'].apply(lambda x: correct_old_characters(x))\n","\n","whitelist = set('abcçdefgğhıijklmnoöpqrsştuüvwxyz ABCÇDEFGĞHIİJKLMNOÖPQRSŞTUÜVWXYZ')\n","\n","test_set['remove_non_tr'] = test_set['correct_chars'].apply(lambda x: ''.join(filter(whitelist.__contains__, x)))\n","test_set['remove_2_ch'] = test_set['remove_non_tr'].apply(remove_two_ch_words)\n","test_set['lower_ch'] = test_set['remove_2_ch'].apply(lambda x: lower(x))\n","test_set['strip_spaces'] = test_set['lower_ch'].apply(gsp.strip_multiple_whitespaces)\n","test_set['find_lemmas'] = test_set['strip_spaces'].apply(lemmatizer)\n","test_set.dropna(axis=0, inplace=True)"],"metadata":{"id":"Z4WMswMrAsMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_set.head()"],"metadata":{"id":"_EDMjJfPSrNl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Offensive Olup Olmaması Tahmini"],"metadata":{"id":"5ah1S0ra5mt5"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"idjvAqg36hru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","# Load the trained model\n","model = AutoModel.from_pretrained('METEoRS/berturk_model_isoffensive')\n","tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"],"metadata":{"id":"jHL2BqTT5rir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the test inputs\n","max_length = 64\n","test_input = test_set[\"strip_spaces\"].tolist()\n","\n","# Tokenize test input\n","tokenized_inputs = tokenizer(test_input, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Convert tokenized inputs to PyTorch tensors\n","test_input_ids = tokenized_inputs[\"input_ids\"]\n","test_attention_mask = tokenized_inputs[\"attention_mask\"]\n","\n","# Evaluate the model on test inputs\n","with torch.no_grad():\n","    outputs = model(test_input_ids, test_attention_mask, return_dict=False)\n","\n","\n","predictions = torch.sigmoid(outputs[0]).round().tolist()\n","print(predictions)"],"metadata":{"id":"IQlPlZck579P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Aşağılayıcı Söylem Tahmini"],"metadata":{"id":"0_cY6dnQ6AI7"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","# Load the trained model\n","model = AutoModel.from_pretrained('METEoRS/berturk_model_insult')\n","tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"],"metadata":{"id":"hII1tI5i6eWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = {0: \"OTHER\", 1: \"INSULT\", 2: \"PROFANITY\", 3: \"SEXIST\", 4: \"RACIST\"}\n","\n","# Define the test inputs\n","max_length = 64\n","test_input = test_set[\"strip_spaces\"].tolist()\n","\n","# Tokenize test input\n","tokenized_inputs = tokenizer(test_input, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Convert tokenized inputs to PyTorch tensors\n","test_input_ids = tokenized_inputs[\"input_ids\"]\n","test_attention_mask = tokenized_inputs[\"attention_mask\"]"],"metadata":{"id":"faMJKND-62B9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tahminleri alın\n","outputs = model(test_input_ids, test_attention_mask)\n","predicted_labels = torch.argmax(outputs.logits, dim=1)\n","predictions = [label_dict[prediction] for prediction in predicted_labels]\n","\n","# Tahminleri yazdırın\n","print(predictions)"],"metadata":{"id":"TnjNWPwo925-"},"execution_count":null,"outputs":[]}]}